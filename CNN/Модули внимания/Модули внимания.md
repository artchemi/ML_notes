---
noteID: dad70594-3509-4dbd-8a9b-2f4d0b4779dc
tags:
  - Attention
---
Механизм [Attention](https://arxiv.org/abs/1807.06521) (внимания) призван подсказывать сети, куда смотреть, но также и улучшать представление того, на чем фокусируется. Поскольку конволюция, извлекая признаки, смешивает канальную и пространственную информацию, мы применим модуль, чтобы выделить важное по этим двум направлениям отдельно (два отдельных модуля внимания еще и требует меньше дополнительных параметров и вычислений).

Модуль получается легким и широко применимым, при этом он заметно улучшает результаты по сравнению с обычными сверточными сетями.
Получая на вход feature map размерности `C×H×W` модуль производит `C×1×1` карту внимания для каналов ($M_c$) и `1×H×W` карту внимания для пространственных данных ($M_s$).

## [[Channel Attention Module]]

$$M_c = \sigma(MLP(AvgPool(F)) + MLP(MaxPool(F)))$$
## [[Spatial attention module]]

$$M_s(F) = \sigma(f^{7×7}(AvgPool(F), MaxPool(F)))$$

---

![[Pasted image 20250612154517.png]]
![[Pasted image 20250612155128.png]]

## [[Squeeze-and-Excitation Networks]]




# Ссылки
1. [Хабр](https://habr.com/ru/articles/527984/)
2. [ИТМО Викиконспекты](https://neerc.ifmo.ru/wiki/index.php?title=Механизм_внимания)
3. [CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)
4. [Efficient Channel Attention](https://arxiv.org/abs/1910.03151)
5. 