---
noteID: 87892b0d-3e22-43c5-bad9-1113decf8285
tags:
  - Attention
---
**Канальный модуль внимания** (англ. _channel attention module_) реализуется за счет исследования внутриканальных взаимосвязей во входных данных, то есть пытается извлечь информацию из яркости каналов одного пикселя. Фокусируется на том, "какая" информация находится в данных. Для более эффективной реализации используется сжатие входных данных по измерениям $H$ и $W$ с помощью [пулингов](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8#.D0.9F.D1.83.D0.BB.D0.B8.D0.BD.D0.B3.D0.BE.D0.B2.D1.8B.D0.B9_.D1.81.D0.BB.D0.BE.D0.B9 "Сверточные нейронные сети") `MaxPool` и `AvgPool`, которые применяются независимо к входному тензору. В результате которого получаются два вектора $F^c_{max}$и $F^c_{avg}$ из $\mathbb{R}^C$. После чего к этим двум векторам независимо применяется одна и та же [полносвязная нейронная сеть](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8,_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD#.D0.9C.D0.BD.D0.BE.D0.B3.D0.BE.D1.81.D0.BB.D0.BE.D0.B9.D0.BD.D1.8B.D0.B5_.D0.BD.D0.B5.D0.B9.D1.80.D0.BE.D0.BD.D0.BD.D1.8B.D0.B5_.D1.81.D0.B5.D1.82.D0.B8 "Нейронные сети, перцептрон") с одним скрытым слоем малой размерности (при этом ее входные и выходные вектора принадлежат $\mathbb{R}^C$). После этого полученные из нейросети вектора поэлементно складываются, к результату поэлементно применяется сигмоидная функция активации и добавляются недостающие единичные размерности. Полученный тензор из $\mathbb{R}^{C×1×1}$ как раз и является результатом применения $A_1(F)$, поэлементное произведение которого со входом $F$ дает тензор $F1$.
![[Pasted image 20250612150222.png]]
Итого результат этой модели можно представить в виде:
### $$M_c = \sigma(MLP(AvgPool(F)) + MLP(MaxPool(F)))$$
